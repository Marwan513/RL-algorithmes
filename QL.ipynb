{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time # to get the time\n",
    "import sys\n",
    "\n",
    "env = gym.make('MountainCar-v0',render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "num_episodes = 60000\n",
    "alpha = 0.15\n",
    "epsilon = 0.15\n",
    "\n",
    "# Set Q-table\n",
    "nA = env.action_space.n\n",
    "nS = env.observation_space.shape[0]\n",
    "np.random.seed(3)\n",
    "Observation = [100,100]\n",
    "Q = np.random.uniform(low=0, high=1, size=(Observation + [nA]))\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state, n_bins=(100, 100)):\n",
    "    \"\"\"\n",
    "    Convert the continuous state values to discrete values.\n",
    "\n",
    "    Parameters:\n",
    "        state (np.ndarray): The current state of the environment.\n",
    "        n_bins (tuple): The number of bins to use for each state variable.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The discrete state representation.\n",
    "    \"\"\"\n",
    "    # Define the bounds for each state variable\n",
    "    bounds = [\n",
    "        [-1.2, 0.5],  # cart position\n",
    "        [-0.07, 0.07],  # cart velocity\n",
    "    ]\n",
    "    \n",
    "    # Calculate the bin width for each state variable\n",
    "    bin_widths = [(bounds[i][1] - bounds[i][0]) / n_bins[i] for i in range(len(bounds))]\n",
    "    # Convert each state variable to a discrete value\n",
    "    discrete_state = tuple(int((state[i] - bounds[i][0]) / bin_widths[i]) for i in range(len(bounds)))\n",
    "    \n",
    "    # Make sure the discrete state is within the bounds of the Q-table\n",
    "    for i in range(len(bounds)):\n",
    "        if discrete_state[i] < 0:\n",
    "            discrete_state = list(discrete_state)\n",
    "            discrete_state[i] = 0\n",
    "            discrete_state = tuple(discrete_state)\n",
    "        elif discrete_state[i] >= n_bins[i]:\n",
    "            discrete_state = list(discrete_state)\n",
    "            discrete_state[i] = n_bins[i] - 1\n",
    "            discrete_state = tuple(discrete_state)\n",
    "    \n",
    "    return discrete_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epsilon-greedy policy\n",
    "def epsilon_greedy(Q, state, nA, epsilon):\n",
    "    if np.random.random() > epsilon:\n",
    "        return np.argmax(Q[state])\n",
    "    else:\n",
    "        return np.random.choice(nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60000/60000.\n"
     ]
    }
   ],
   "source": [
    "# Q-learning algorithm\n",
    "total_reward = 0\n",
    "total_time = 0\n",
    "max_time = 0\n",
    "min_time = 1e+20\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    episode_reward = 0\n",
    "\n",
    "    t0 = time.time() #set the initial time\n",
    "    state = env.reset(seed=32)[0]\n",
    "    d_state = get_discrete_state(state)\n",
    "    action = epsilon_greedy(Q, d_state, nA, epsilon)\n",
    "    terminated, truncated = False,False\n",
    "    \n",
    "    while not (terminated or truncated):\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        if abs(state[1]) >= abs(next_state[1]):\n",
    "            reward = -10\n",
    "        if next_state[0] >= 0.5:\n",
    "            reward = 50\n",
    "        next_state = get_discrete_state(next_state)\n",
    "        next_action = epsilon_greedy(Q, next_state, nA, epsilon)\n",
    "        Q[d_state][action] += alpha*(reward + np.max(Q[next_state]) - Q[d_state][action])\n",
    "        d_state = next_state\n",
    "        action = next_action\n",
    "        episode_reward += reward\n",
    "\n",
    "    t1 = time.time() #episode has finished\n",
    "    episode_time = t1 - t0 #episode total time\n",
    "    if episode_time > max_time:\n",
    "        max_time = episode_time\n",
    "    if episode_time < min_time:\n",
    "        min_time = episode_time\n",
    "    total_time += episode_time\n",
    "    total_reward += episode_reward #episode total reward\n",
    "    if i_episode % 10 == 0:\n",
    "        print(f\"\\rEpisode {i_episode}/{num_episodes}.\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -128.13848333333334\n",
      "Time Average: 0.014800279188156127\n",
      "Biggist ep time: 0.1319575309753418\n",
      "smallest ep time: 0.0069925785064697266\n"
     ]
    }
   ],
   "source": [
    "mean_reward = total_reward / num_episodes\n",
    "mean_time = total_time / num_episodes\n",
    "\n",
    "print(\"Mean Reward: \" + str(mean_reward))\n",
    "print(\"Time Average: \" + str(mean_time))\n",
    "print(\"Biggist ep time: \" + str(max_time))\n",
    "print(\"smallest ep time: \" + str(min_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0',render_mode=\"rgb_array_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of action: 124\n",
      "Moviepy - Building video c:\\Users\\marwa\\Desktop\\task3\\videos/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video c:\\Users\\marwa\\Desktop\\task3\\videos/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready c:\\Users\\marwa\\Desktop\\task3\\videos/rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from gym.utils.save_video import save_video\n",
    "# Evaluate learned policy\n",
    "state = env.reset(seed=32)\n",
    "state = state[0]\n",
    "state = get_discrete_state(state)\n",
    "terminated, truncated = False,False\n",
    "counter = 0\n",
    "while not (terminated or truncated):\n",
    "    action = np.argmax(Q[state])\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    next_state = get_discrete_state(next_state)\n",
    "    state = next_state\n",
    "    counter += 1\n",
    "    \n",
    "print(\"number of action: \" + str(counter))\n",
    "\n",
    "save_video(\n",
    "  env.render(),\n",
    "  \"videos\",\n",
    "  fps=35,\n",
    "  episode_index=1\n",
    ")\n",
    "\n",
    "# env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
